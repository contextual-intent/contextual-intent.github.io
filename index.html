<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/css/78ccb5d22d250a31.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/eee61b64c3c37e41.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-b734be9c8027a33f.js"/><script src="/_next/static/chunks/fd9d1056-ab27ff52827ea900.js" async=""></script><script src="/_next/static/chunks/117-7b97b7d03429596e.js" async=""></script><script src="/_next/static/chunks/main-app-4223782ade9ce4d7.js" async=""></script><script src="/_next/static/chunks/ca377847-7e987621759529a8.js" async=""></script><script src="/_next/static/chunks/7a49ec60-358c752022298c41.js" async=""></script><script src="/_next/static/chunks/233-e3246d4cf323f42c.js" async=""></script><script src="/_next/static/chunks/343-a9086327813d0eea.js" async=""></script><script src="/_next/static/chunks/app/page-78460eec5d8b1c74.js" async=""></script><script src="/_next/static/chunks/app/layout-8cfbc9894cc7529e.js" async=""></script><title>Grounding Agent Memory in Contextual Intent</title><meta name="description" content="Research paper on STITCH: A system for grounding agent memory in contextual intent using structured retrieval cues and intent indexing."/><meta name="author" content="Research Team"/><meta name="keywords" content="AI,LLM,Agent Memory,Contextual Intent,STITCH,CAME-Bench"/><meta property="og:title" content="Grounding Agent Memory in Contextual Intent"/><meta property="og:description" content="Research paper on STITCH: A system for grounding agent memory in contextual intent"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Grounding Agent Memory in Contextual Intent"/><meta name="twitter:description" content="Research paper on STITCH: A system for grounding agent memory in contextual intent"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body><nav class="fixed top-0 left-0 right-0 z-50 transition-all duration-300 bg-white/80 backdrop-blur-sm"><div class="container mx-auto px-4 sm:px-6 lg:px-8"><div class="flex items-center justify-between h-16 md:h-20"><a class="flex items-center gap-2 md:gap-3 hover:opacity-80 transition-opacity group" href="/"><div class="flex-shrink-0"><img alt="UIUC Logo" loading="lazy" width="40" height="40" decoding="async" data-nimg="1" class="w-8 h-8 md:w-10 md:h-10 object-contain" style="color:transparent" src="/images/uiuc_logo.png"/></div><div class="flex flex-col leading-tight"><span class="text-sm md:text-base font-bold text-[#13294B] tracking-wide">UIUC DMG</span><span class="text-[10px] md:text-xs text-gray-600 font-medium">Data Mining Group</span></div></a><div class="hidden md:flex items-center space-x-1 lg:space-x-2"><a class="px-3 lg:px-4 py-2 text-sm lg:text-base text-gray-700 hover:text-blue-600 transition-colors font-medium" href="/#abstract">Abstract</a><a class="px-3 lg:px-4 py-2 text-sm lg:text-base text-gray-700 hover:text-blue-600 transition-colors font-medium" href="/#findings">Key Findings</a><a class="px-3 lg:px-4 py-2 text-sm lg:text-base text-gray-700 hover:text-blue-600 transition-colors font-medium" href="/#methodology">Methodology</a><a class="px-3 lg:px-4 py-2 text-sm lg:text-base text-gray-700 hover:text-blue-600 transition-colors font-medium" href="/#benchmark">Benchmark</a><a class="px-3 lg:px-4 py-2 text-sm lg:text-base text-gray-700 hover:text-blue-600 transition-colors font-medium" href="/visualizer">Data Explorer</a><a target="_blank" rel="noopener noreferrer" class="px-3 lg:px-4 py-2 text-sm lg:text-base text-gray-700 hover:text-blue-600 transition-colors font-medium" href="https://arxiv.org/abs/2601.10702">Research Paper</a></div><button class="md:hidden p-2 text-gray-700 hover:text-blue-600 transition-colors" aria-label="Toggle menu"><svg class="w-6 h-6" fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" stroke="currentColor"><path d="M4 6h16M4 12h16M4 18h16"></path></svg></button></div></div></nav><main class="min-h-screen pt-16 md:pt-20"><section class="relative w-full min-h-screen flex items-center justify-center bg-gradient-to-br from-blue-50 via-white to-purple-50"><div class="container mx-auto px-4 sm:px-6 lg:px-8 py-20"><div class="max-w-5xl mx-auto text-center"><h1 class="text-4xl sm:text-5xl md:text-6xl lg:text-7xl font-bold text-gray-900 mb-6 leading-tight">Grounding Agent Memory in Contextual Intent</h1><div class="mb-6"><div class="flex flex-wrap justify-center items-center gap-x-4 gap-y-2 text-lg md:text-xl font-semibold text-gray-900"><a href="https://www.linkedin.com/in/ruozhen-yang/" target="_blank" rel="noopener noreferrer" class="hover:text-blue-600 transition-colors">Ruozhen Yang</a><a href="https://cs.stanford.edu/~yuchengj/" target="_blank" rel="noopener noreferrer" class="hover:text-blue-600 transition-colors">Yucheng Jiang</a><a href="https://www.linkedin.com/in/yueqijiang/" target="_blank" rel="noopener noreferrer" class="hover:text-blue-600 transition-colors">Yueqi Jiang</a><a href="https://pkargupta.github.io" target="_blank" rel="noopener noreferrer" class="hover:text-blue-600 transition-colors">Priyanka Kargupta</a><a href="https://yzhan238.github.io" target="_blank" rel="noopener noreferrer" class="hover:text-blue-600 transition-colors">Yunyi Zhang</a><a href="http://hanj.cs.illinois.edu" target="_blank" rel="noopener noreferrer" class="hover:text-blue-600 transition-colors">Jiawei Han</a></div></div><div class="flex flex-col sm:flex-row gap-4 justify-center items-center mt-12 flex-wrap"><a target="_blank" rel="noopener noreferrer" class="px-6 py-3 bg-white text-[#B31B1B] border-2 border-[#B31B1B] rounded-lg font-semibold text-lg hover:bg-red-50 transition-colors shadow-lg hover:shadow-xl flex items-center gap-2" href="https://arxiv.org/abs/2601.10702"><svg class="w-5 h-5" viewBox="0 0 17.732 24.269" xmlns="http://www.w3.org/2000/svg" role="img" aria-label="arXiv Logo"><g><path d="M573.549,280.916l2.266,2.738,6.674-7.84c.353-.47.52-.717.353-1.117a1.218,1.218,0,0,0-1.061-.748h0a.953.953,0,0,0-.712.262Z" transform="translate(-566.984 -271.548)" fill="currentColor"></path><path d="M579.525,282.225l-10.606-10.174a1.413,1.413,0,0,0-.834-.5,1.09,1.09,0,0,0-1.027.66c-.167.4-.047.681.319,1.206l8.44,10.242h0l-6.282,7.716a1.336,1.336,0,0,0-.323,1.3,1.114,1.114,0,0,0,1.04.69A.992.992,0,0,0,571,293l8.519-7.92A1.924,1.924,0,0,0,579.525,282.225Z" transform="translate(-566.984 -271.548)" fill="currentColor"></path><path d="M584.32,293.912l-8.525-10.275,0,0L573.53,280.9l-1.389,1.254a2.063,2.063,0,0,0,0,2.965l10.812,10.419a.925.925,0,0,0,.742.282,1.039,1.039,0,0,0,.953-.667A1.261,1.261,0,0,0,584.32,293.912Z" transform="translate(-566.984 -271.548)" fill="currentColor"></path></g></svg>Paper</a><a target="_blank" rel="noopener noreferrer" class="px-6 py-3 bg-gray-900 text-white rounded-lg font-semibold text-lg hover:bg-gray-800 transition-colors shadow-lg hover:shadow-xl flex items-center gap-2" href="https://github.com/Seattleyrz/contextual-intent"><svg class="w-5 h-5" fill="currentColor" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M12 2C6.477 2 2 6.484 2 12.017c0 4.425 2.865 8.18 6.839 9.504.5.092.682-.217.682-.483 0-.237-.008-.868-.013-1.703-2.782.605-3.369-1.343-3.369-1.343-.454-1.158-1.11-1.466-1.11-1.466-.908-.62.069-.608.069-.608 1.003.07 1.531 1.032 1.531 1.032.892 1.53 2.341 1.088 2.91.832.092-.647.35-1.088.636-1.338-2.22-.253-4.555-1.113-4.555-4.951 0-1.093.39-1.988 1.029-2.688-.103-.253-.446-1.272.098-2.65 0 0 .84-.27 2.75 1.026A9.564 9.564 0 0112 6.844c.85.004 1.705.115 2.504.337 1.909-1.296 2.747-1.027 2.747-1.027.546 1.379.202 2.398.1 2.651.64.7 1.028 1.595 1.028 2.688 0 3.848-2.339 4.695-4.566 4.943.359.309.678.92.678 1.855 0 1.338-.012 2.419-.012 2.747 0 .268.18.58.688.482A10.019 10.019 0 0022 12.017C22 6.484 17.522 2 12 2z" clip-rule="evenodd"></path></svg>Code</a><a target="_blank" rel="noopener noreferrer" class="px-6 py-3 bg-[#FFD21E] text-gray-900 rounded-lg font-semibold text-lg hover:bg-[#F5C518] transition-colors shadow-lg hover:shadow-xl flex items-center gap-2" href="https://huggingface.co/datasets/Seattleyrz/CAME-Bench"><span class="text-xl">ü§ó</span>Benchmark Data</a><a class="px-6 py-3 bg-white text-blue-600 border-2 border-blue-600 rounded-lg font-semibold text-lg hover:bg-blue-50 transition-colors flex items-center gap-2" href="#abstract">Learn More</a></div></div><div class="mt-12 max-w-4xl mx-auto"><div class="relative w-full rounded-lg overflow-hidden shadow-2xl bg-transparent"><div class="relative w-full flex items-center justify-center object-contain"><div class="absolute inset-0 bg-gray-100 animate-pulse flex items-center justify-center z-10" style="min-height:400px"><p class="text-gray-400 text-sm">Loading PDF...</p></div><div class="react-pdf__Document w-full flex items-center justify-center" style="--scale-factor:1"><div class="react-pdf__message react-pdf__message--loading"></div></div></div></div></div></div></section><section id="abstract" class="py-20 bg-white scroll-mt-16 md:scroll-mt-20"><div class="container mx-auto px-4 sm:px-6 lg:px-8"><div class="max-w-5xl mx-auto"><h2 class="text-4xl font-bold text-gray-900 mb-8">Abstract</h2><div class="prose prose-lg max-w-none text-gray-700 leading-relaxed"><p class="mb-6">Deploying large language models in long-horizon, goal-oriented interactions remains challenging because similar entities and facts recur under different latent goals and constraints, causing memory systems to retrieve context-mismatched evidence. We propose <strong class="text-blue-600">STITCH</strong> (Structured Intent Tracking in Contextual History), an agentic memory system that indexes each trajectory step with a structured retrieval cue, contextual intent, and retrieves history by matching the current step‚Äôs intent. Contextual intent provides compact signals that disambiguate repeated mentions and reduce interference: (1) the current latent goal defining a thematic segment, (2) the action type, and (3) the salient entity types anchoring which attributes matter. During inference, STITCH filters and prioritizes memory snippets by intent compatibility, suppressing semantically similar but context-incompatible history.</p><p class="mb-6">For evaluation, we introduce <strong class="text-blue-600">CAME-Bench</strong>, a benchmark for context-aware retrieval in realistic, dynamic, goal-oriented trajectories. Across CAME-Bench and LongMemEval, STITCH achieves state-of-the-art performance, outperforming the strongest baseline by 35.6%, with the largest gains as trajectory length increases. Our analysis shows that intent indexing substantially reduces retrieval noise, supporting intent-aware memory for robust long-horizon reasoning.</p></div></div></div></section><section id="findings" class="py-20 bg-gray-50 scroll-mt-16 md:scroll-mt-20"><div class="container mx-auto px-4 sm:px-6 lg:px-8"><div class="max-w-6xl mx-auto"><h2 class="text-4xl font-bold text-gray-900 mb-12 text-center">Key Findings</h2><div class="grid grid-cols-1 md:grid-cols-2 gap-8 mb-12"><div class="bg-white p-8 rounded-lg shadow-md hover:shadow-xl hover:-translate-y-1 transition-all duration-300"><div class="flex items-start gap-4"><div class="flex-shrink-0 w-12 h-12 bg-gray-900 text-white rounded-full flex items-center justify-center font-bold text-xl shadow-sm">1</div><div><h3 class="text-xl font-semibold text-gray-900 mb-3">State-of-the-Art Performance</h3><p class="text-gray-700 leading-relaxed">STITCH consistently outperforms 13 strong baselines‚Äîincluding long-context LLMs (GPT-4.1-mini, GPT-5-mini) and structured memory systems (GraphRAG, RAPTOR)‚Äîacross both CAME-Bench and LongMemEval benchmarks.</p></div></div></div><div class="bg-white p-8 rounded-lg shadow-md hover:shadow-xl hover:-translate-y-1 transition-all duration-300"><div class="flex items-start gap-4"><div class="flex-shrink-0 w-12 h-12 bg-gray-900 text-white rounded-full flex items-center justify-center font-bold text-xl shadow-sm">2</div><div><h3 class="text-xl font-semibold text-gray-900 mb-3">Scales to Long Horizons</h3><p class="text-gray-700 leading-relaxed">While standard baselines degrade as trajectory length increases, STITCH remains robust. On the &#x27;Large&#x27; subset (~408k tokens), it outperforms the strongest baseline by 35.6%, eliminating the &#x27;lost-in-the-middle&#x27; phenomenon.</p></div></div></div><div class="bg-white p-8 rounded-lg shadow-md hover:shadow-xl hover:-translate-y-1 transition-all duration-300"><div class="flex items-start gap-4"><div class="flex-shrink-0 w-12 h-12 bg-gray-900 text-white rounded-full flex items-center justify-center font-bold text-xl shadow-sm">3</div><div><h3 class="text-xl font-semibold text-gray-900 mb-3">Eliminates Retrieval Noise</h3><p class="text-gray-700 leading-relaxed">By indexing steps with Contextual Intent, STITCH suppresses &#x27;distractors&#x27;‚Äîevidence that is semantically similar but belongs to a different goal or time‚Äîensuring the agent retrieves the right fact in the right context.</p></div></div></div><div class="bg-white p-8 rounded-lg shadow-md hover:shadow-xl hover:-translate-y-1 transition-all duration-300"><div class="flex items-start gap-4"><div class="flex-shrink-0 w-12 h-12 bg-gray-900 text-white rounded-full flex items-center justify-center font-bold text-xl shadow-sm">4</div><div><h3 class="text-xl font-semibold text-gray-900 mb-3">Thematic Scope is Critical</h3><p class="text-gray-700 leading-relaxed">Ablation studies reveal that &#x27;Thematic Scope&#x27; (goal-oriented segmentation) provides the largest performance gain, effectively partitioning long interaction histories into manageable, coherent episodes.</p></div></div></div></div><div class="mt-12 w-full"><div class="w-full bg-white rounded-lg shadow-lg p-6"><div class="flex flex-col sm:flex-row justify-between items-center mb-6 gap-4"><div class="flex bg-gray-100 p-1 rounded-lg"><button class="px-4 py-2 rounded-md text-sm font-medium transition-all bg-white text-black shadow-sm">CAME-Bench</button><button class="px-4 py-2 rounded-md text-sm font-medium transition-all text-gray-500 hover:text-gray-900">LongMemEval</button></div><div class="flex items-center gap-4"><label class="flex items-center gap-2 cursor-pointer text-sm text-gray-700 select-none"><input type="checkbox" class="rounded border-gray-300 text-blue-600 focus:ring-blue-500" checked=""/>Show All Models</label></div></div><div class="flex items-center justify-center gap-4 mb-4"><span class="text-sm font-medium text-gray-700">Toggle Subsets:</span><div class="flex flex-wrap gap-2 justify-center"><button title="Click to hide Short Subset (23k)" class="px-3 py-1.5 text-xs font-medium rounded-full border transition-all duration-200 flex items-center gap-2 select-none bg-gray-800 text-white border-gray-800 hover:bg-gray-700 shadow-sm"><span class="w-2 h-2 rounded-full bg-white"></span>Short Subset (23k)</button><button title="Click to hide Medium Subset (137k)" class="px-3 py-1.5 text-xs font-medium rounded-full border transition-all duration-200 flex items-center gap-2 select-none bg-gray-800 text-white border-gray-800 hover:bg-gray-700 shadow-sm"><span class="w-2 h-2 rounded-full bg-white"></span>Medium Subset (137k)</button><button title="Click to hide Long Subset (408k)" class="px-3 py-1.5 text-xs font-medium rounded-full border transition-all duration-200 flex items-center gap-2 select-none bg-gray-800 text-white border-gray-800 hover:bg-gray-700 shadow-sm"><span class="w-2 h-2 rounded-full bg-white"></span>Long Subset (408k)</button></div></div><div class="flex flex-wrap gap-4 text-xs text-gray-600 mb-2 justify-end"><span class="font-medium mr-1">Categories:</span><div class="flex items-center gap-1"><span class="w-2 h-2 rounded-full bg-gray-700"></span><span>Long Context</span></div><div class="flex items-center gap-1"><span class="w-2 h-2 rounded-full bg-green-500"></span><span>Embedding RAG</span></div><div class="flex items-center gap-1"><span class="w-2 h-2 rounded-full bg-blue-500"></span><span>Structure RAG</span></div><div class="flex items-center gap-1"><span class="w-2 h-2 rounded-full bg-orange-600"></span><span class="font-bold text-orange-600">Ours</span></div></div><div class="relative h-[600px] w-full"><canvas role="img" height="150" width="300"></canvas></div><p class="text-xs text-gray-500 mt-4 text-center">* Metrics shown: Macro-F1 for CAME-Bench, Accuracy for LongMemEval.</p></div></div></div></div></section><section id="methodology" class="py-20 bg-white scroll-mt-16 md:scroll-mt-20"><div class="container mx-auto px-4 sm:px-6 lg:px-8"><div class="max-w-5xl mx-auto"><h2 class="text-4xl font-bold text-gray-900 mb-12">Methodology (STITCH)</h2><div class="space-y-16"><div><p class="text-gray-700 leading-relaxed mb-6">Standard retrieval relies on semantic similarity, which fails when similar facts recur in different contexts.<span class="font-semibold"> STITCH</span> (Structured Intent Tracking in Contextual History) solves this by indexing every trajectory step with a structured retrieval cue called <span class="font-semibold">Contextual Intent</span>. Grounded in Event Structure Theory, STITCH organizes memory not as a flat list, but as a structured history of goals, actions, and entities.</p><div class="mb-6"><h4 class="text-xl font-semibold text-gray-900 mb-4">Core Concepts</h4><div class="space-y-4"><div class="bg-blue-50 p-6 rounded-lg hover:shadow-lg hover:-translate-y-1 transition-all duration-300"><h5 class="font-semibold text-gray-900 mb-2">The Schema: Contextual Intent</h5><p class="text-gray-700">We index each step with a tuple <span class="font-mono">Œπt = (œÉt, œµt, Œ∫t)</span>:<span class="block mt-2">‚Ä¢ <span class="font-semibold">Thematic Scope (Partonomy):</span> A stable label tracking the current high-level goal (e.g., ‚ÄúDay 2 Itinerary‚Äù).<br/>‚Ä¢ <span class="font-semibold">Event Type (Taxonomy):</span> An action label capturing the operation performed (e.g., ‚ÄúComparison‚Äù, ‚ÄúDebugging‚Äù).<br/>‚Ä¢ <span class="font-semibold">Key Entity Types (Taxonomy):</span> The schema class identifying relevant attributes (e.g., ‚ÄúPrice‚Äù, ‚ÄúError Code‚Äù).</span></p></div><div class="bg-purple-50 p-6 rounded-lg hover:shadow-lg hover:-translate-y-1 transition-all duration-300"><h5 class="font-semibold text-gray-900 mb-2">Online Induction (Domain Agnostic)</h5><p class="text-gray-700">STITCH does not require a pre-defined ontology. It induces these cues <strong>online</strong> from the streaming trajectory. It uses sliding windows to detect goal shifts and maintains dynamic vocabularies for event and entity types that evolve as the agent encounters new tasks.</p></div><div class="bg-green-50 p-6 rounded-lg hover:shadow-lg hover:-translate-y-1 transition-all duration-300"><h5 class="font-semibold text-gray-900 mb-2">Structural Coreference Resolution</h5><p class="text-gray-700">Implicit references are a major cause of retrieval failure. STITCH leverages the induced structure to resolve references<em>before</em> storage. For example, an ambiguous step like <span class="italic">&quot;Book it&quot;</span> is rewritten to <span class="font-semibold">&quot;Book the Apollo Hotel&quot;</span> by aligning it with the active thematic scope.</p></div><div class="bg-slate-50 p-6 rounded-lg hover:shadow-lg hover:-translate-y-1 transition-all duration-300 border border-slate-200"><h5 class="font-semibold text-gray-900 mb-2">Retrieval: Structure First, Semantics Second</h5><p class="text-gray-700">At inference time, STITCH translates a query into a structured filter. We employ <strong>Label Density Ranking</strong>, which strictly prioritizes memory snippets that match the <em>intent structure</em> first. Semantic similarity is used only as a tie-breaker, effectively filtering out contextually irrelevant distractors.</p></div></div></div><div class="mt-8"><h4 class="text-xl font-semibold text-gray-900 mb-4">System Architecture</h4><div class="relative w-full rounded-lg overflow-hidden shadow-lg bg-transparent"><div class="relative w-full"><img alt="STITCH Overview - Contextual Intent Construction and Intent-Aware Retrieval" loading="lazy" width="1200" height="900" decoding="async" data-nimg="1" class="object-contain w-full h-auto" style="color:transparent;max-width:100%;height:auto" src="/images/method_figure.jpg"/><div class="absolute inset-0 bg-gray-100 animate-pulse flex items-center justify-center"><p class="text-gray-400 text-sm">Loading...</p></div></div></div></div></div><div id="benchmark" class="scroll-mt-16 md:scroll-mt-20"><div class="flex flex-col gap-4 sm:flex-row sm:items-center sm:justify-between mb-6"><h3 class="text-3xl font-semibold text-gray-900">The Benchmark: CAME-Bench</h3><a href="/visualizer" class="inline-flex items-center justify-center rounded-lg bg-gray-900 px-6 py-3 text-sm font-semibold text-white hover:bg-gray-800 transition-colors shadow-md hover:shadow-lg">Open Benchmark Explorer ‚Üí</a></div><p class="text-gray-700 leading-relaxed mb-6">Existing benchmarks often rely on unrealistic turn-taking or independent topics. To rigorously test context-aware retrieval, we introduce <strong class="text-blue-600">CAME-Bench</strong>. It features continuous, goal-oriented trajectories constructed with <strong>High Contextual Interference</strong>‚Äîwhere recurring entities and interleaved goals create significant ambiguity.</p><div class="mb-6"><h4 class="text-xl font-semibold text-gray-900 mb-4">Design Principles</h4><div class="space-y-4"><div class="bg-orange-50 p-6 rounded-lg hover:shadow-lg hover:-translate-y-1 transition-all duration-300"><h5 class="font-semibold text-gray-900 mb-2">Symbolically Grounded Consistency</h5><p class="text-gray-700">We decouple planning from generation. A symbolic planner ensures logical causal consistency over long horizons (e.g., <span class="font-semibold">Travel Planning</span> itineraries and <span class="font-semibold">Debate</span> argumentation), guaranteeing that every state transition is valid before it is rendered into natural language.</p></div><div class="bg-yellow-50 p-6 rounded-lg hover:shadow-lg hover:-translate-y-1 transition-all duration-300"><h5 class="font-semibold text-gray-900 mb-2">Controlled Semantic Interference</h5><p class="text-gray-700">We utilize a closed-world environment to <strong>densely reuse static entities</strong>. This forces models to disambiguate fine-grained context (e.g., the same hotel appearing in three different potential plans) rather than relying on unique keywords.</p></div><div class="bg-pink-50 p-6 rounded-lg hover:shadow-lg hover:-translate-y-1 transition-all duration-300"><h5 class="font-semibold text-gray-900 mb-2">Dynamic Referential Ambiguity</h5><p class="text-gray-700">Interactions are not strictly turn-taking. Requests are often interleaved, deferred, or implicitly referenced later (e.g., &quot;Use the evidence from that previous counter-argument&quot;), requiring the memory system to track state updates rather than static facts.</p></div></div></div><div class="mt-10"><h4 class="text-xl font-semibold text-gray-900 mb-4">Evaluation Capabilities</h4><p class="text-gray-700 leading-relaxed mb-6">We evaluate four distinct capabilities required for robust long-horizon agents:</p><div class="grid grid-cols-1 md:grid-cols-2 gap-4"><div class="bg-slate-50 p-6 rounded-lg hover:shadow-lg hover:-translate-y-1 transition-all duration-300 border border-slate-200"><h5 class="font-semibold text-gray-900 mb-2">1. Incremental Memory Revision</h5><p class="text-gray-700 text-sm">Can the agent track state changes? <br/><em>Ex: Tracking a restaurant candidate list as items are added and subsequently rejected across turns.</em></p></div><div class="bg-slate-50 p-6 rounded-lg hover:shadow-lg hover:-translate-y-1 transition-all duration-300 border border-slate-200"><h5 class="font-semibold text-gray-900 mb-2">2. Context-Aware Factual Recall</h5><p class="text-gray-700 text-sm">Can the agent disambiguate similar facts? <br/><em>Ex: Retrieving the hotel price specifically for &quot;Day 2&quot;, distinguishing it from the &quot;Day 1&quot; price.</em></p></div><div class="bg-slate-50 p-6 rounded-lg hover:shadow-lg hover:-translate-y-1 transition-all duration-300 border border-slate-200"><h5 class="font-semibold text-gray-900 mb-2">3. Multi-Hop Reasoning</h5><p class="text-gray-700 text-sm">Can the agent resolve implicit references? <br/><em>Ex: Identifying what &quot;that reservation&quot; refers to, then retrieving its associated attributes.</em></p></div><div class="bg-slate-50 p-6 rounded-lg hover:shadow-lg hover:-translate-y-1 transition-all duration-300 border border-slate-200"><h5 class="font-semibold text-gray-900 mb-2">4. Information Synthesis</h5><p class="text-gray-700 text-sm">Can the agent aggregate scattered info? <br/><em>Ex: Reconstructing a full 3-day itinerary from bookings scattered across 500 turns of dialogue.</em></p></div></div></div></div></div></div></div></section></main><script src="/_next/static/chunks/webpack-b734be9c8027a33f.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/css/78ccb5d22d250a31.css\",\"style\"]\n2:HL[\"/_next/static/css/eee61b64c3c37e41.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"3:I[2846,[],\"\"]\n5:I[2972,[\"674\",\"static/chunks/ca377847-7e987621759529a8.js\",\"651\",\"static/chunks/7a49ec60-358c752022298c41.js\",\"233\",\"static/chunks/233-e3246d4cf323f42c.js\",\"343\",\"static/chunks/343-a9086327813d0eea.js\",\"931\",\"static/chunks/app/page-78460eec5d8b1c74.js\"],\"\"]\n6:I[5390,[\"674\",\"static/chunks/ca377847-7e987621759529a8.js\",\"651\",\"static/chunks/7a49ec60-358c752022298c41.js\",\"233\",\"static/chunks/233-e3246d4cf323f42c.js\",\"343\",\"static/chunks/343-a9086327813d0eea.js\",\"931\",\"static/chunks/app/page-78460eec5d8b1c74.js\"],\"default\"]\n7:I[190,[\"674\",\"static/chunks/ca377847-7e987621759529a8.js\",\"651\",\"static/chunks/7a49ec60-358c752022298c41.js\",\"233\",\"static/chunks/233-e3246d4cf323f42c.js\",\"343\",\"static/chunks/343-a9086327813d0eea.js\",\"931\",\"static/chunks/app/page-78460eec5d8b1c74.js\"],\"default\"]\n8:I[282,[\"674\",\"static/chunks/ca377847-7e987621759529a8.js\",\"651\",\"static/chunks/7a49ec60-358c752022298c41.js\",\"233\",\"static/chunks/233-e3246d4cf323f42c.js\",\"343\",\"static/chunks/343-a9086327813d0eea.js\",\"931\",\"static/chunks/app/page-78460eec5d8b1c74.js\"],\"default\"]\n9:I[2343,[\"233\",\"static/chunks/233-e3246d4cf323f42c.js\",\"185\",\"static/chunks/app/layout-8cfbc9894cc7529e.js\"],\"default\"]\na:I[4707,[],\"\"]\nb:I[6423,[],\"\"]\nd:I[1060,[],\"\"]\ne:[]\n"])</script><script>self.__next_f.push([1,"0:[\"$\",\"$L3\",null,{\"buildId\":\"SejjlN0KLbxs_6vfRs3uT\",\"assetPrefix\":\"\",\"urlParts\":[\"\",\"\"],\"initialTree\":[\"\",{\"children\":[\"__PAGE__\",{}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"__PAGE__\",{},[[\"$L4\",[\"$\",\"main\",null,{\"className\":\"min-h-screen pt-16 md:pt-20\",\"children\":[[\"$\",\"section\",null,{\"className\":\"relative w-full min-h-screen flex items-center justify-center bg-gradient-to-br from-blue-50 via-white to-purple-50\",\"children\":[\"$\",\"div\",null,{\"className\":\"container mx-auto px-4 sm:px-6 lg:px-8 py-20\",\"children\":[[\"$\",\"div\",null,{\"className\":\"max-w-5xl mx-auto text-center\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-4xl sm:text-5xl md:text-6xl lg:text-7xl font-bold text-gray-900 mb-6 leading-tight\",\"children\":\"Grounding Agent Memory in Contextual Intent\"}],[\"$\",\"div\",null,{\"className\":\"mb-6\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-wrap justify-center items-center gap-x-4 gap-y-2 text-lg md:text-xl font-semibold text-gray-900\",\"children\":[[\"$\",\"a\",\"0\",{\"href\":\"https://www.linkedin.com/in/ruozhen-yang/\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"hover:text-blue-600 transition-colors\",\"children\":\"Ruozhen Yang\"}],[\"$\",\"a\",\"1\",{\"href\":\"https://cs.stanford.edu/~yuchengj/\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"hover:text-blue-600 transition-colors\",\"children\":\"Yucheng Jiang\"}],[\"$\",\"a\",\"2\",{\"href\":\"https://www.linkedin.com/in/yueqijiang/\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"hover:text-blue-600 transition-colors\",\"children\":\"Yueqi Jiang\"}],[\"$\",\"a\",\"3\",{\"href\":\"https://pkargupta.github.io\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"hover:text-blue-600 transition-colors\",\"children\":\"Priyanka Kargupta\"}],[\"$\",\"a\",\"4\",{\"href\":\"https://yzhan238.github.io\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"hover:text-blue-600 transition-colors\",\"children\":\"Yunyi Zhang\"}],[\"$\",\"a\",\"5\",{\"href\":\"http://hanj.cs.illinois.edu\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"hover:text-blue-600 transition-colors\",\"children\":\"Jiawei Han\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"flex flex-col sm:flex-row gap-4 justify-center items-center mt-12 flex-wrap\",\"children\":[[\"$\",\"$L5\",null,{\"href\":\"https://arxiv.org/abs/2601.10702\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"px-6 py-3 bg-white text-[#B31B1B] border-2 border-[#B31B1B] rounded-lg font-semibold text-lg hover:bg-red-50 transition-colors shadow-lg hover:shadow-xl flex items-center gap-2\",\"children\":[[\"$\",\"svg\",null,{\"className\":\"w-5 h-5\",\"viewBox\":\"0 0 17.732 24.269\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"role\":\"img\",\"aria-label\":\"arXiv Logo\",\"children\":[\"$\",\"g\",null,{\"children\":[[\"$\",\"path\",null,{\"d\":\"M573.549,280.916l2.266,2.738,6.674-7.84c.353-.47.52-.717.353-1.117a1.218,1.218,0,0,0-1.061-.748h0a.953.953,0,0,0-.712.262Z\",\"transform\":\"translate(-566.984 -271.548)\",\"fill\":\"currentColor\"}],[\"$\",\"path\",null,{\"d\":\"M579.525,282.225l-10.606-10.174a1.413,1.413,0,0,0-.834-.5,1.09,1.09,0,0,0-1.027.66c-.167.4-.047.681.319,1.206l8.44,10.242h0l-6.282,7.716a1.336,1.336,0,0,0-.323,1.3,1.114,1.114,0,0,0,1.04.69A.992.992,0,0,0,571,293l8.519-7.92A1.924,1.924,0,0,0,579.525,282.225Z\",\"transform\":\"translate(-566.984 -271.548)\",\"fill\":\"currentColor\"}],[\"$\",\"path\",null,{\"d\":\"M584.32,293.912l-8.525-10.275,0,0L573.53,280.9l-1.389,1.254a2.063,2.063,0,0,0,0,2.965l10.812,10.419a.925.925,0,0,0,.742.282,1.039,1.039,0,0,0,.953-.667A1.261,1.261,0,0,0,584.32,293.912Z\",\"transform\":\"translate(-566.984 -271.548)\",\"fill\":\"currentColor\"}]]}]}],\"Paper\"]}],[\"$\",\"$L5\",null,{\"href\":\"https://github.com/Seattleyrz/contextual-intent\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"px-6 py-3 bg-gray-900 text-white rounded-lg font-semibold text-lg hover:bg-gray-800 transition-colors shadow-lg hover:shadow-xl flex items-center gap-2\",\"children\":[[\"$\",\"svg\",null,{\"className\":\"w-5 h-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 24 24\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"path\",null,{\"fillRule\":\"evenodd\",\"d\":\"M12 2C6.477 2 2 6.484 2 12.017c0 4.425 2.865 8.18 6.839 9.504.5.092.682-.217.682-.483 0-.237-.008-.868-.013-1.703-2.782.605-3.369-1.343-3.369-1.343-.454-1.158-1.11-1.466-1.11-1.466-.908-.62.069-.608.069-.608 1.003.07 1.531 1.032 1.531 1.032.892 1.53 2.341 1.088 2.91.832.092-.647.35-1.088.636-1.338-2.22-.253-4.555-1.113-4.555-4.951 0-1.093.39-1.988 1.029-2.688-.103-.253-.446-1.272.098-2.65 0 0 .84-.27 2.75 1.026A9.564 9.564 0 0112 6.844c.85.004 1.705.115 2.504.337 1.909-1.296 2.747-1.027 2.747-1.027.546 1.379.202 2.398.1 2.651.64.7 1.028 1.595 1.028 2.688 0 3.848-2.339 4.695-4.566 4.943.359.309.678.92.678 1.855 0 1.338-.012 2.419-.012 2.747 0 .268.18.58.688.482A10.019 10.019 0 0022 12.017C22 6.484 17.522 2 12 2z\",\"clipRule\":\"evenodd\"}]}],\"Code\"]}],[\"$\",\"$L5\",null,{\"href\":\"https://huggingface.co/datasets/Seattleyrz/CAME-Bench\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"px-6 py-3 bg-[#FFD21E] text-gray-900 rounded-lg font-semibold text-lg hover:bg-[#F5C518] transition-colors shadow-lg hover:shadow-xl flex items-center gap-2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xl\",\"children\":\"ü§ó\"}],\"Benchmark Data\"]}],[\"$\",\"$L5\",null,{\"href\":\"#abstract\",\"className\":\"px-6 py-3 bg-white text-blue-600 border-2 border-blue-600 rounded-lg font-semibold text-lg hover:bg-blue-50 transition-colors flex items-center gap-2\",\"children\":\"Learn More\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"mt-12 max-w-4xl mx-auto\",\"children\":[\"$\",\"div\",null,{\"className\":\"relative w-full rounded-lg overflow-hidden shadow-2xl bg-transparent\",\"children\":[\"$\",\"$L6\",null,{\"src\":\"/images/stitch_web_main.pdf\",\"alt\":\"Project Overview\",\"className\":\"object-contain\"}]}]}]]}]}],[\"$\",\"section\",null,{\"id\":\"abstract\",\"className\":\"py-20 bg-white scroll-mt-16 md:scroll-mt-20\",\"children\":[\"$\",\"div\",null,{\"className\":\"container mx-auto px-4 sm:px-6 lg:px-8\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-5xl mx-auto\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-4xl font-bold text-gray-900 mb-8\",\"children\":\"Abstract\"}],[\"$\",\"div\",null,{\"className\":\"prose prose-lg max-w-none text-gray-700 leading-relaxed\",\"children\":[[\"$\",\"p\",null,{\"className\":\"mb-6\",\"children\":[\"Deploying large language models in long-horizon, goal-oriented interactions remains challenging because similar entities and facts recur under different latent goals and constraints, causing memory systems to retrieve context-mismatched evidence. We propose \",[\"$\",\"strong\",null,{\"className\":\"text-blue-600\",\"children\":\"STITCH\"}],\" (Structured Intent Tracking in Contextual History), an agentic memory system that indexes each trajectory step with a structured retrieval cue, contextual intent, and retrieves history by matching the current step‚Äôs intent. Contextual intent provides compact signals that disambiguate repeated mentions and reduce interference: (1) the current latent goal defining a thematic segment, (2) the action type, and (3) the salient entity types anchoring which attributes matter. During inference, STITCH filters and prioritizes memory snippets by intent compatibility, suppressing semantically similar but context-incompatible history.\"]}],[\"$\",\"p\",null,{\"className\":\"mb-6\",\"children\":[\"For evaluation, we introduce \",[\"$\",\"strong\",null,{\"className\":\"text-blue-600\",\"children\":\"CAME-Bench\"}],\", a benchmark for context-aware retrieval in realistic, dynamic, goal-oriented trajectories. Across CAME-Bench and LongMemEval, STITCH achieves state-of-the-art performance, outperforming the strongest baseline by 35.6%, with the largest gains as trajectory length increases. Our analysis shows that intent indexing substantially reduces retrieval noise, supporting intent-aware memory for robust long-horizon reasoning.\"]}]]}]]}]}]}],[\"$\",\"section\",null,{\"id\":\"findings\",\"className\":\"py-20 bg-gray-50 scroll-mt-16 md:scroll-mt-20\",\"children\":[\"$\",\"div\",null,{\"className\":\"container mx-auto px-4 sm:px-6 lg:px-8\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-6xl mx-auto\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-4xl font-bold text-gray-900 mb-12 text-center\",\"children\":\"Key Findings\"}],[\"$\",\"div\",null,{\"className\":\"grid grid-cols-1 md:grid-cols-2 gap-8 mb-12\",\"children\":[[\"$\",\"div\",\"1\",{\"className\":\"bg-white p-8 rounded-lg shadow-md hover:shadow-xl hover:-translate-y-1 transition-all duration-300\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex items-start gap-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex-shrink-0 w-12 h-12 bg-gray-900 text-white rounded-full flex items-center justify-center font-bold text-xl shadow-sm\",\"children\":\"1\"}],[\"$\",\"div\",null,{\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-xl font-semibold text-gray-900 mb-3\",\"children\":\"State-of-the-Art Performance\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-700 leading-relaxed\",\"children\":\"STITCH consistently outperforms 13 strong baselines‚Äîincluding long-context LLMs (GPT-4.1-mini, GPT-5-mini) and structured memory systems (GraphRAG, RAPTOR)‚Äîacross both CAME-Bench and LongMemEval benchmarks.\"}]]}]]}]}],[\"$\",\"div\",\"2\",{\"className\":\"bg-white p-8 rounded-lg shadow-md hover:shadow-xl hover:-translate-y-1 transition-all duration-300\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex items-start gap-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex-shrink-0 w-12 h-12 bg-gray-900 text-white rounded-full flex items-center justify-center font-bold text-xl shadow-sm\",\"children\":\"2\"}],[\"$\",\"div\",null,{\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-xl font-semibold text-gray-900 mb-3\",\"children\":\"Scales to Long Horizons\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-700 leading-relaxed\",\"children\":\"While standard baselines degrade as trajectory length increases, STITCH remains robust. On the 'Large' subset (~408k tokens), it outperforms the strongest baseline by 35.6%, eliminating the 'lost-in-the-middle' phenomenon.\"}]]}]]}]}],[\"$\",\"div\",\"3\",{\"className\":\"bg-white p-8 rounded-lg shadow-md hover:shadow-xl hover:-translate-y-1 transition-all duration-300\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex items-start gap-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex-shrink-0 w-12 h-12 bg-gray-900 text-white rounded-full flex items-center justify-center font-bold text-xl shadow-sm\",\"children\":\"3\"}],[\"$\",\"div\",null,{\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-xl font-semibold text-gray-900 mb-3\",\"children\":\"Eliminates Retrieval Noise\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-700 leading-relaxed\",\"children\":\"By indexing steps with Contextual Intent, STITCH suppresses 'distractors'‚Äîevidence that is semantically similar but belongs to a different goal or time‚Äîensuring the agent retrieves the right fact in the right context.\"}]]}]]}]}],[\"$\",\"div\",\"4\",{\"className\":\"bg-white p-8 rounded-lg shadow-md hover:shadow-xl hover:-translate-y-1 transition-all duration-300\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex items-start gap-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex-shrink-0 w-12 h-12 bg-gray-900 text-white rounded-full flex items-center justify-center font-bold text-xl shadow-sm\",\"children\":\"4\"}],[\"$\",\"div\",null,{\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-xl font-semibold text-gray-900 mb-3\",\"children\":\"Thematic Scope is Critical\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-700 leading-relaxed\",\"children\":\"Ablation studies reveal that 'Thematic Scope' (goal-oriented segmentation) provides the largest performance gain, effectively partitioning long interaction histories into manageable, coherent episodes.\"}]]}]]}]}]]}],[\"$\",\"div\",null,{\"className\":\"mt-12 w-full\",\"children\":[\"$\",\"$L7\",null,{}]}]]}]}]}],[\"$\",\"section\",null,{\"id\":\"methodology\",\"className\":\"py-20 bg-white scroll-mt-16 md:scroll-mt-20\",\"children\":[\"$\",\"div\",null,{\"className\":\"container mx-auto px-4 sm:px-6 lg:px-8\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-5xl mx-auto\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-4xl font-bold text-gray-900 mb-12\",\"children\":\"Methodology (STITCH)\"}],[\"$\",\"div\",null,{\"className\":\"space-y-16\",\"children\":[[\"$\",\"div\",null,{\"children\":[[\"$\",\"p\",null,{\"className\":\"text-gray-700 leading-relaxed mb-6\",\"children\":[\"Standard retrieval relies on semantic similarity, which fails when similar facts recur in different contexts.\",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\" STITCH\"}],\" (Structured Intent Tracking in Contextual History) solves this by indexing every trajectory step with a structured retrieval cue called \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"Contextual Intent\"}],\". Grounded in Event Structure Theory, STITCH organizes memory not as a flat list, but as a structured history of goals, actions, and entities.\"]}],[\"$\",\"div\",null,{\"className\":\"mb-6\",\"children\":[[\"$\",\"h4\",null,{\"className\":\"text-xl font-semibold text-gray-900 mb-4\",\"children\":\"Core Concepts\"}],[\"$\",\"div\",null,{\"className\":\"space-y-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"bg-blue-50 p-6 rounded-lg hover:shadow-lg hover:-translate-y-1 transition-all duration-300\",\"children\":[[\"$\",\"h5\",null,{\"className\":\"font-semibold text-gray-900 mb-2\",\"children\":\"The Schema: Contextual Intent\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-700\",\"children\":[\"We index each step with a tuple \",[\"$\",\"span\",null,{\"className\":\"font-mono\",\"children\":\"Œπt = (œÉt, œµt, Œ∫t)\"}],\":\",[\"$\",\"span\",null,{\"className\":\"block mt-2\",\"children\":[\"‚Ä¢ \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"Thematic Scope (Partonomy):\"}],\" A stable label tracking the current high-level goal (e.g., ‚ÄúDay 2 Itinerary‚Äù).\",[\"$\",\"br\",null,{}],\"‚Ä¢ \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"Event Type (Taxonomy):\"}],\" An action label capturing the operation performed (e.g., ‚ÄúComparison‚Äù, ‚ÄúDebugging‚Äù).\",[\"$\",\"br\",null,{}],\"‚Ä¢ \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"Key Entity Types (Taxonomy):\"}],\" The schema class identifying relevant attributes (e.g., ‚ÄúPrice‚Äù, ‚ÄúError Code‚Äù).\"]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"bg-purple-50 p-6 rounded-lg hover:shadow-lg hover:-translate-y-1 transition-all duration-300\",\"children\":[[\"$\",\"h5\",null,{\"className\":\"font-semibold text-gray-900 mb-2\",\"children\":\"Online Induction (Domain Agnostic)\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-700\",\"children\":[\"STITCH does not require a pre-defined ontology. It induces these cues \",[\"$\",\"strong\",null,{\"children\":\"online\"}],\" from the streaming trajectory. It uses sliding windows to detect goal shifts and maintains dynamic vocabularies for event and entity types that evolve as the agent encounters new tasks.\"]}]]}],[\"$\",\"div\",null,{\"className\":\"bg-green-50 p-6 rounded-lg hover:shadow-lg hover:-translate-y-1 transition-all duration-300\",\"children\":[[\"$\",\"h5\",null,{\"className\":\"font-semibold text-gray-900 mb-2\",\"children\":\"Structural Coreference Resolution\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-700\",\"children\":[\"Implicit references are a major cause of retrieval failure. STITCH leverages the induced structure to resolve references\",[\"$\",\"em\",null,{\"children\":\"before\"}],\" storage. For example, an ambiguous step like \",[\"$\",\"span\",null,{\"className\":\"italic\",\"children\":\"\\\"Book it\\\"\"}],\" is rewritten to \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"\\\"Book the Apollo Hotel\\\"\"}],\" by aligning it with the active thematic scope.\"]}]]}],[\"$\",\"div\",null,{\"className\":\"bg-slate-50 p-6 rounded-lg hover:shadow-lg hover:-translate-y-1 transition-all duration-300 border border-slate-200\",\"children\":[[\"$\",\"h5\",null,{\"className\":\"font-semibold text-gray-900 mb-2\",\"children\":\"Retrieval: Structure First, Semantics Second\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-700\",\"children\":[\"At inference time, STITCH translates a query into a structured filter. We employ \",[\"$\",\"strong\",null,{\"children\":\"Label Density Ranking\"}],\", which strictly prioritizes memory snippets that match the \",[\"$\",\"em\",null,{\"children\":\"intent structure\"}],\" first. Semantic similarity is used only as a tie-breaker, effectively filtering out contextually irrelevant distractors.\"]}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"mt-8\",\"children\":[[\"$\",\"h4\",null,{\"className\":\"text-xl font-semibold text-gray-900 mb-4\",\"children\":\"System Architecture\"}],[\"$\",\"div\",null,{\"className\":\"relative w-full rounded-lg overflow-hidden shadow-lg bg-transparent\",\"children\":[\"$\",\"$L8\",null,{\"src\":\"/images/method_figure.jpg\",\"alt\":\"STITCH Overview - Contextual Intent Construction and Intent-Aware Retrieval\",\"placeholder\":\"[Architecture Diagram: Construction Phase vs. Retrieval Phase]\",\"width\":1200,\"height\":900,\"className\":\"w-full h-auto\"}]}]]}]]}],[\"$\",\"div\",null,{\"id\":\"benchmark\",\"className\":\"scroll-mt-16 md:scroll-mt-20\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-col gap-4 sm:flex-row sm:items-center sm:justify-between mb-6\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-3xl font-semibold text-gray-900\",\"children\":\"The Benchmark: CAME-Bench\"}],[\"$\",\"a\",null,{\"href\":\"/visualizer\",\"className\":\"inline-flex items-center justify-center rounded-lg bg-gray-900 px-6 py-3 text-sm font-semibold text-white hover:bg-gray-800 transition-colors shadow-md hover:shadow-lg\",\"children\":\"Open Benchmark Explorer ‚Üí\"}]]}],[\"$\",\"p\",null,{\"className\":\"text-gray-700 leading-relaxed mb-6\",\"children\":[\"Existing benchmarks often rely on unrealistic turn-taking or independent topics. To rigorously test context-aware retrieval, we introduce \",[\"$\",\"strong\",null,{\"className\":\"text-blue-600\",\"children\":\"CAME-Bench\"}],\". It features continuous, goal-oriented trajectories constructed with \",[\"$\",\"strong\",null,{\"children\":\"High Contextual Interference\"}],\"‚Äîwhere recurring entities and interleaved goals create significant ambiguity.\"]}],[\"$\",\"div\",null,{\"className\":\"mb-6\",\"children\":[[\"$\",\"h4\",null,{\"className\":\"text-xl font-semibold text-gray-900 mb-4\",\"children\":\"Design Principles\"}],[\"$\",\"div\",null,{\"className\":\"space-y-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"bg-orange-50 p-6 rounded-lg hover:shadow-lg hover:-translate-y-1 transition-all duration-300\",\"children\":[[\"$\",\"h5\",null,{\"className\":\"font-semibold text-gray-900 mb-2\",\"children\":\"Symbolically Grounded Consistency\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-700\",\"children\":[\"We decouple planning from generation. A symbolic planner ensures logical causal consistency over long horizons (e.g., \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"Travel Planning\"}],\" itineraries and \",[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"Debate\"}],\" argumentation), guaranteeing that every state transition is valid before it is rendered into natural language.\"]}]]}],[\"$\",\"div\",null,{\"className\":\"bg-yellow-50 p-6 rounded-lg hover:shadow-lg hover:-translate-y-1 transition-all duration-300\",\"children\":[[\"$\",\"h5\",null,{\"className\":\"font-semibold text-gray-900 mb-2\",\"children\":\"Controlled Semantic Interference\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-700\",\"children\":[\"We utilize a closed-world environment to \",[\"$\",\"strong\",null,{\"children\":\"densely reuse static entities\"}],\". This forces models to disambiguate fine-grained context (e.g., the same hotel appearing in three different potential plans) rather than relying on unique keywords.\"]}]]}],[\"$\",\"div\",null,{\"className\":\"bg-pink-50 p-6 rounded-lg hover:shadow-lg hover:-translate-y-1 transition-all duration-300\",\"children\":[[\"$\",\"h5\",null,{\"className\":\"font-semibold text-gray-900 mb-2\",\"children\":\"Dynamic Referential Ambiguity\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-700\",\"children\":\"Interactions are not strictly turn-taking. Requests are often interleaved, deferred, or implicitly referenced later (e.g., \\\"Use the evidence from that previous counter-argument\\\"), requiring the memory system to track state updates rather than static facts.\"}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"mt-10\",\"children\":[[\"$\",\"h4\",null,{\"className\":\"text-xl font-semibold text-gray-900 mb-4\",\"children\":\"Evaluation Capabilities\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-700 leading-relaxed mb-6\",\"children\":\"We evaluate four distinct capabilities required for robust long-horizon agents:\"}],[\"$\",\"div\",null,{\"className\":\"grid grid-cols-1 md:grid-cols-2 gap-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"bg-slate-50 p-6 rounded-lg hover:shadow-lg hover:-translate-y-1 transition-all duration-300 border border-slate-200\",\"children\":[[\"$\",\"h5\",null,{\"className\":\"font-semibold text-gray-900 mb-2\",\"children\":\"1. Incremental Memory Revision\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-700 text-sm\",\"children\":[\"Can the agent track state changes? \",[\"$\",\"br\",null,{}],[\"$\",\"em\",null,{\"children\":\"Ex: Tracking a restaurant candidate list as items are added and subsequently rejected across turns.\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"bg-slate-50 p-6 rounded-lg hover:shadow-lg hover:-translate-y-1 transition-all duration-300 border border-slate-200\",\"children\":[[\"$\",\"h5\",null,{\"className\":\"font-semibold text-gray-900 mb-2\",\"children\":\"2. Context-Aware Factual Recall\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-700 text-sm\",\"children\":[\"Can the agent disambiguate similar facts? \",[\"$\",\"br\",null,{}],[\"$\",\"em\",null,{\"children\":\"Ex: Retrieving the hotel price specifically for \\\"Day 2\\\", distinguishing it from the \\\"Day 1\\\" price.\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"bg-slate-50 p-6 rounded-lg hover:shadow-lg hover:-translate-y-1 transition-all duration-300 border border-slate-200\",\"children\":[[\"$\",\"h5\",null,{\"className\":\"font-semibold text-gray-900 mb-2\",\"children\":\"3. Multi-Hop Reasoning\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-700 text-sm\",\"children\":[\"Can the agent resolve implicit references? \",[\"$\",\"br\",null,{}],[\"$\",\"em\",null,{\"children\":\"Ex: Identifying what \\\"that reservation\\\" refers to, then retrieving its associated attributes.\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"bg-slate-50 p-6 rounded-lg hover:shadow-lg hover:-translate-y-1 transition-all duration-300 border border-slate-200\",\"children\":[[\"$\",\"h5\",null,{\"className\":\"font-semibold text-gray-900 mb-2\",\"children\":\"4. Information Synthesis\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-700 text-sm\",\"children\":[\"Can the agent aggregate scattered info? \",[\"$\",\"br\",null,{}],[\"$\",\"em\",null,{\"children\":\"Ex: Reconstructing a full 3-day itinerary from bookings scattered across 500 turns of dialogue.\"}]]}]]}]]}]]}]]}]]}]]}]}]}]]}],[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/eee61b64c3c37e41.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]]],null],null]},[[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/78ccb5d22d250a31.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"children\":[[\"$\",\"$L9\",null,{}],[\"$\",\"$La\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Lb\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[]}]]}]}]],null],null],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$Lc\"],\"globalErrorComponent\":\"$d\",\"missingSlots\":\"$We\"}]\n"])</script><script>self.__next_f.push([1,"c:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"Grounding Agent Memory in Contextual Intent\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Research paper on STITCH: A system for grounding agent memory in contextual intent using structured retrieval cues and intent indexing.\"}],[\"$\",\"meta\",\"4\",{\"name\":\"author\",\"content\":\"Research Team\"}],[\"$\",\"meta\",\"5\",{\"name\":\"keywords\",\"content\":\"AI,LLM,Agent Memory,Contextual Intent,STITCH,CAME-Bench\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:title\",\"content\":\"Grounding Agent Memory in Contextual Intent\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:description\",\"content\":\"Research paper on STITCH: A system for grounding agent memory in contextual intent\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"9\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"10\",{\"name\":\"twitter:title\",\"content\":\"Grounding Agent Memory in Contextual Intent\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:description\",\"content\":\"Research paper on STITCH: A system for grounding agent memory in contextual intent\"}]]\n4:null\n"])</script></body></html>